Session 1:
-----------

1.) The "Why Kubernetes?" - Limitations of Docker Alone


Docker's Strengths:

    - Dockerfile: A blueprint or script containing instructions to build a Docker image. It specifies everything needed for your application (e.g., an e-commerce website) to run.
    
    - Image: A read-only template created from a Dockerfile. It's a build-time construct. Think of it as a packaged application with all its dependencies.
    
    - Container: A runnable instance of an image. It's a runtime construct. Think of it as a running process based on the image.
    
Challenges with Docker in Production (without an orchestrator):
    
    - Multi-Containers: While docker-compose is excellent for defining and running multi-container applications on a single Docker host, it's not designed for managing applications across a cluster of machines.

    - Cluster Management: Coordinating and managing containers across multiple machines.

    - Scaling: Manual Scaling is Tedious and Error-Prone: If your application needs more instances due to increased load, you'd have to manually docker run new containers. If these need to be on different servers, you'd SSH into each, pull the image, run the container, and configure networking. This doesn't scale for dozens or hundreds of instances.

    - Load Balancing: How do you distribute incoming traffic evenly across your multiple container instances? You'd need to manually configure an external load balancer (like Nginx, HAProxy, or a cloud load balancer) and update its configuration every time you add or remove a container instance. This is slow and prone to misconfiguration.

    - Resource Allocation: How do you decide which server to run a new container on to best utilize available CPU and memory? This becomes a manual, complex decision process without an orchestrator intelligently scheduling workloads.

    - No Automated Horizontal Scaling: Docker itself has no built-in mechanism to automatically increase or decrease the number of container instances based on load (e.g., CPU utilization or incoming requests).
    
    - Ephemeral Nature & Self-Healing: Docker's restart policies (always, unless-stopped, etc.) can restart a container if the container process itself exits. But what if the container is running, but the application inside it is frozen, in a deadlock, or consistently returning errors? Docker alone doesn't have sophisticated application-level health checks to detect this state and take corrective action

    - Service Discovery: How do containers find and talk to each other when their IPs can change due to restart?

    - Automated Rollouts & Rollbacks: Safely deploying new versions of your application without downtime.

    - Declarative State Management: Defining the desired state of your application (e.g., "I want 5 instances of this web server running version 1.2") and having a system work to achieve and maintain that state.


All the above problems are resolved using Container orchestrator like Kubernetes

-----

2.) Managing Applications at Scale using Kubernetes

    - Real world Production Workloads e.g., amazon.com often:
        
        - Run on a cluster of servers to handle load and ensure fault tolerance:

            Handling Load (Scalability): A single server, no matter how powerful, has finite resources (CPU, RAM, network bandwidth). As user traffic and data processing needs grow, the application load can exceed the capacity of one machine. A cluster allows the workload to be distributed across multiple servers.

            Ensuring Fault Tolerance (High Availability): Hardware fails, software crashes, networks glitch. If your entire application runs on a single server, any failure on that server means your application goes down. By running on a cluster, if one server (or "node") fails, the others can continue to operate and potentially take over the workload of the failed node, minimizing or preventing downtime.
        
        - Are built as a collection of microservices (e.g., amazon.com might have 40+ services like checkout, order, frontend, API gateway):

            Complexity Management: Large, monolithic applications become increasingly difficult to understand, develop, maintain, and update. Breaking them down into smaller, independent services (microservices) makes each part more manageable.

            Independent Deployment & Scaling: Each microservice (e.g., checkout-service, order-service, user-profile-service, product-catalog-service, frontend-ui, api-gateway for an e-commerce site) can be developed, deployed, and scaled independently. If the product-catalog-service needs more resources due to high browsing activity, it can be scaled up without affecting the checkout-service.
    
        - These microservices need to communicate with each other (e.g., frontend talks to an API, which talks to a backend service etc.)

            Functional Decomposition: Microservices are individual pieces of a larger puzzle. To fulfill a complete user request or business process, they need to collaborate.

            Data Flow and Orchestration: For example, when a user places an order:
                - The frontend-ui microservice might send a request to the api-gateway.
                - The api-gateway might route it to the order-service.
                - The order-service might need to communicate with the inventory-service (to check stock), the user-profile-service (to get shipping details), and then the payment-service (to process payment).

            This inter-service communication is critical for the application to function as a cohesive whole.

    - How Kubernetes Helps: Kubernetes is specifically designed to address these production workload complexities.

        - For Cluster Management & Resource Distribution:
            
            Kubernetes excels at managing a pool of worker nodes (your servers) and distributing your application components (as Pods) across them.
            
            It intelligently schedules workloads based on resource availability and defined constraints, ensuring efficient use of the cluster.
    
            It handles node failures by automatically attempting to reschedule workloads from a failed node to healthy ones, thus maintaining application availability.

        - For Managing Microservices:
            
            Kubernetes is ideally suited for deploying and managing containerized microservices. 
            
            Each microservice can be packaged into a container and then managed by Kubernetes constructs like Deployments.

            This allows for easy, independent scaling of each microservice (e.g., "I need 3 instances of order-service but 10 instances of product-catalog-service").
            
            It facilitates rolling updates and rollbacks for individual microservices, allowing for zero-downtime deployments.

        - For Inter-Service Communication:
            
            Kubernetes provides robust mechanisms for service discovery and internal load balancing through its Service abstraction.

            Each microservice (or set of its Pods) can be exposed via a stable internal IP address and DNS name (e.g., order-service.namespace.svc.cluster.local).

            This means one microservice can reliably find and communicate with another, even as individual Pods (instances of those services) are created, destroyed, or moved across nodes, as their IP addresses are dynamic.

-----

3.) Core Kubernetes Concepts for Application Management:

    - In a microservices environment deployed on Kubernetes, each microservice typically runs in its own set of Pods (managed by a Deployment).
    
    - A Pod can run multiple containers. This is often used for "sidecar" patterns, where a main application container is accompanied by helper/supporting containers (e.g., for logging, monitoring, proxying).

    - ReplicaSet ensures a specified number of Pod replicas are running at any given time. If you state you need 5 replicas of your application, the ReplicaSet works to maintain 5 running Pods.

    - Hierarchy: You define a Deployment -> which creates and manages a ReplicaSet -> which creates and manages Pods.

    - Labels and Selectors:
        
        Labels: Key/value pairs attached to Kubernetes objects (especially Pods).
        
        Selectors: Used to identify and select objects (especially pods) based on their labels.
        
        This is the core mechanism Kubernetes uses to connect different resources. For example, a Deployment uses selectors to know which Pods it's managing, and a Service uses selectors to know which Pods to send traffic to.

    - Namespace provides a way to divide cluster into virtual sub-cluster creating logical isolation within a physical cluster.

    - In microservice env whenever it is deployed on K8S, each microservices should always run in a separate pod / deployment.

-----

4.) Kubernetes Networking: Enabling Communication

Scenario: You have 10 microservices to deploy in a Kubernetes cluster.
- 2 microservices need to be exposed externally to end-users.
- All 10 microservices need to talk to each other within the cluster.

How to achieve this?

Discussion:

    - Pod IPs - The Challenge:
        
        Every Pod gets its own unique IP address within the cluster (viewable with kubectl get pods -n <namespace> -o wide).

        Technically, microservices (running in Pods) could communicate via these Pod IPs.
        
        Problem: Pods are ephemeral. 
        
            If a Pod (e.g., one of 3 checkout-MS Pods) dies, its Deployment/ReplicaSet will create a new one. This new Pod will get a new IP address.
            
            If other microservices (e.g., order-MS) were trying to communicate with the old IP, that communication would fail. Managing these dynamic IPs for inter-service communication is extremely complex and not done in real-world scenarios.

    - Kubernetes Services - The Solution for Stable Communication:
            
            Services provide a stable abstraction (a fixed IP address and DNS name) over a dynamic set of Pods.

            ClusterIP Service (for internal communication): This is the default Service type.
                - Exposes the Service on an internal IP address, only reachable from within the cluster.
                - Acts as an internal load balancer, distributing traffic to the Pods matching its selector.
                - Provides a fixed, stable IP address and a DNS name (e.g., checkout-service.your-namespace.svc.cluster.local) for the service. Microservices use this stable address/name to communicate, regardless of individual Pod IPs changing.


Solution:

    - For our scenario: 
        - The 8 microservices that only need internal communication, and all 10 for inter-service communication, would use ClusterIP Services.

        - The 2 externally exposed microservices could use.:
            - NodePort Service (for basic external access):
                - Exposes the Service on a static port (the NodePort, typically in the 30000-32767 range) on each Worker Node's IP address.
                - Allows external traffic to reach the service via http://<WorkerNodeIP>:<NodePort>.
    
                Example: If your cluster has worker nodes with IPs 112.23.14.57 to 112.23.14.61 and your NodePort is 30080, you can access the service via 112.23.14.57:30080, 112.23.14.58:30080, etc.

            - LoadBalancer Service (for robust external access, usually cloud-provider specific):
                - Exposes the Service externally using a cloud provider's load balancer (e.g., AWS ELB, Azure Load Balancer, GCP Cloud Load Balancer).
                
                - The cloud provider provisions an external load balancer.

        For our scenario: Load Balancer service type is often the preferred method for the 2 externally exposed microservices in a cloud environment.


    - Another option here is: Ingress (for advanced HTTP/S routing):
        - Not a Service type, but an API object that manages external access to services, typically HTTP/S.
        - Provides more advanced features like host-based and path-based routing, SSL/TLS termination.
        - Requires an Ingress Controller (e.g., AWS Load Balancer Controller, Nginx, Traefik, HAProxy) to be running in the cluster.
        
        (details of ingress will be discussed later)
		
----------------------------------------------------------------------------------------------------

Session 2:
-----------

1.) Misconception about Kubernetes Load Balancer:
--------------------------------------------------

Clarification: 
    
    - There isn't a generic "Kubernetes LoadBalancer" object you create directly. Instead, you create a Service of type: LoadBalancer. Kubernetes then interacts with the underlying cloud infrastructure (if available) to provision an actual load balancer.

    - External Access Flow Simplified:
        
        User types https://example.com in his browser.

        Browser queries DNS, which resolves example.com to an IP address. This IP usually points to an external Load Balancer.

        The external Load Balancer forwards traffic into the service running in the Kubernetes cluster 

        The Kubernetes Service inside the cluster then routes the traffic to the correct Pod(s) running the website application.

-----

2.) Configuration Management in Kubernetes

The Need: 

    - Application code often has:
        a.) Business logic.
        b.) Configuration values (database URLs, API keys, feature flags, environment-specific settings).
        c.) The same application code needs to run in different environments (dev, QA, UAT, pre-prod, prod) with different configurations.

    - These configuration files fetch the configuration values from:
        - Env variables
        - External env specific config file, dev-config.yaml, prod-config.yaml
        e.g.: environment-dev.ts, environment-prod.ts

Kubernetes Solutions:

    - ConfigMap: 
        
        Stores non-confidential configuration data as key-value pairs (e.g., database server URL, DB username, service names to connect to).
        
        These can be injected into Pods as environment variables or mounted as files.

    - Secret: 
    
        Stores sensitive data like passwords, API tokens, TLS certificates. Data is stored base64 encoded by default (offering mild obfuscation, not strong encryption at rest unless additional cluster security measures are in place). 
        
        Secrets can also be injected as environment variables or mounted as files.

Important Note:
    
    - A ConfigMap is for injecting configuration data. It cannot directly read files from a Persistent Volume (PV).
    
    - Pods are the entities that perform OS-level operations like reading/writing files. 
    
    - A Pod can mount a ConfigMap (as files) and also mount a PersistentVolumeClaim (PVC, which uses a PV) to access persistent storage.

-----

3.) Scaling Applications in Kubernetes:

    - Manual Scaling: Directly changing the replicas count in a Deployment manifest (e.g., kubectl scale deployment my-app --replicas=10 or changing the replica count in the deployment ).

    - Autoscaling (Dynamic):
        
        -  Horizontal Pod Autoscaler (HPA):

            Automatically scales the number of Pod replicas in a Deployment or ReplicaSet.
            
            Scaling decisions are based on observed metrics like CPU utilization, memory usage, or custom metrics.


        - Vertical Pod Autoscaler (VPA):
            
            Automatically adjusts the CPU and memory resource requests and limits for Pods within a Deployment. (Note: This often involves restarting Pods to apply new resource settings, so it's used more carefully).


4.) Sample Use case: End to end scaling (including pods and worker noes in the cluster)

    - Setup:
        Cluster: 1 control plane, 3 worker nodes.
        Worker node capacity (example): 2 vCPUs, 8GB RAM, 30GB Storage each.
        Application APP10 deployed.
        HPA for APP10: min replicas=2, max replicas=10, target CPU utilization=75%.

    - Event: High traffic hits APP10, average CPU usage across Pods exceeds 75%.
        HPA Action: HPA starts scaling up Pods from 2 towards 10.

    - Problem: Let's say HPA successfully creates 8 new Pods (total 10), but the 9th and 10th Pods go into a Pending state.

    - Reason: Upon checking Pod events (kubectl describe pod <pending-pod-name>), you find messages like "insufficient cpu" or "insufficient memory." The Kubernetes scheduler cannot find any worker nodes with enough available (unallocated) CPU or memory to place these new Pods. The cluster has run out of allocatable resources on the existing nodes.

    - Cluster scaling Solutions:
        Add more worker nodes to the cluster (potentially automated by a Cluster Autoscaler and cloud based auto-scaling solution if on-cloud).
		
----------------------------------------------------------------------------------------------------
	
Session 3:
-----------

1.) Self-Managed Kubernetes Cluster Architecture

	- When you're not using a managed Kubernetes service like EKS, GKE, or AKS, you are responsible for setting up and maintaining all components of the Kubernetes cluster.

	- Control Plane (Master Nodes):
		The brain of the Kubernetes cluster. It makes decisions about the cluster (e.g., scheduling) and detects and responds to cluster events. For production and critical use cases, you may also run multiple control plane nodes / servers for high availability.
		
		- Key Components (running on Control Plane nodes):
			- kube-apiserver: The frontend for the Kubernetes control plane. It exposes the Kubernetes API. All interactions with the cluster (from kubectl, other components, or external tools) go through the API server.

			- etcd: A consistent and highly-available key-value store used as Kubernetes backing store for all cluster data (configurations, state, secrets, etc.). This is critical and losing etcd data means losing the state of your cluster.

			- kube-scheduler: Watches for newly created Pods that have no node assigned and selects a node for them to run on based on resource requirements, policies, and other specifications, etc.

			- controller-manager: Runs controller processes. Controllers are control loops that watch the shared state of the cluster through the apiserver and make changes attempting to move the current state towards the desired state. 
				
				- Examples include:
					- Node Controller: Responsible for noticing and responding when nodes go down.
					 
					- Replication Controller (and ReplicaSet Controller via Deployments): Responsible for maintaining the correct number of pods for every replication controller object in the system.

					- Endpoints Controller: Populates the Endpoints object (i.e., joins Services & Pods).
			
			- cloud-controller-manager (Optional, for cloud provider integration): Embeds cloud-specific control logic. It allows you to link your cluster into your cloud provider's API, and separates out the components that interact with that cloud platform from components that just interact with your cluster. (e.g., managing load balancers, storage volumes). We will not need this in our self managed cluster.
			
	- Worker Nodes:
		These are the machines (VMs or physical servers) where your actual application containers (in Pods) run.

		- Key Components (running on each Worker Node):
			- kubelet: An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod as described by their PodSpecs received from the API server running on the Control Plane. It registers the node with the apiserver.

			- kube-proxy: kube-proxy makes Kubernetes Services (like ClusterIP) work by creating network rules on every node. These rules ensure that when you try to reach a Service's stable IP address, your request gets directed to one of the actual, healthy Pods providing that service.
			
			- Container Runtime Interface (CRI): The software responsible for running containers. Kubernetes is CRI-compatible, meaning you can use various container runtimes.
			Examples: containerd, Docker (via dockershim - being deprecated as a direct kubelet integration), CRI-O.

			- Container Networking Interface (CNI) Plugin: CNI plugins give each Pod its own IP address and set up the network paths so Pods can talk directly to each other, even across different machines. They are essential for basic Pod-to-Pod communication.

-----

2.) Infrastructure Setup for Self-Managed Cluster:
	
	- Virtual Machines (VMs): You will typically provision VMs. 
		For example:
			- 1 or 3 VMs for the Control Plane (odd number for etcd quorum).
			- At least 2 or more VMs for Worker Nodes for basic redundancy and capacity.

	- Networking:

		- VPC / VNet / Intranet: All VMs (control plane and worker nodes) must reside within the same private network (e.g., AWS VPC, Azure VNet, or a corporate intranet) allowing them to communicate with each other.

		- Security Groups (SG) / Network Security Groups (NSG) / Firewalls: These are crucial cloud (or on-prem) constructs that act as virtual firewalls for your VMs. You need to configure rules to:
			- Allow control plane components to talk to each other (especially etcd).
			- Allow worker nodes (kubelet, kube-proxy) to communicate with the kube-apiserver on the control plane.
			- Allow inter-node communication for the CNI plugin (Pod-to-Pod networking overlay).
			- Allow external access to the kube-apiserver for kubectl (usually restricted to specific IPs).
			- Allow inbound traffic for services exposed via NodePort or LoadBalancer.

	- Installation of Kubernetes Components:
		
		- kubeadm: A command-line tool that provides kubeadm init and kubeadm join as best-practice "fast paths" for creating Kubernetes clusters. It bootstraps the control plane and worker nodes by installing and configuring the necessary components. This is the most common method for self-managed clusters.

		- Manifests / Helm: While core Kubernetes components are typically installed by tools like kubeadm or by cloud providers, you use YAML manifests or Helm charts to deploy applications and add-ons (like monitoring tools, ingress controllers, or CNI plugins if not bundled) onto your running cluster.

----------------------------------------------------------------------------------------------------

Session 4:
-----------

1.) Networking Layers: L4 vs L7 Load Balancing

	- Understanding the difference is key to choosing the right type of Kubernetes Service or Ingress.
		- Layer 4 (Transport Layer - TCP/UDP):
			- Operation: Works at the TCP or UDP protocol level. It makes routing decisions based on source/destination IP addresses and ports.
			- Visibility: It does not inspect the actual content (payload) of the network packets. It doesn't understand HTTP headers, paths, or cookies.
			
			- Use Cases: 
				- Simple, fast load balancing for any TCP/UDP based service. Good for high throughput, low latency scenarios where content inspection is not needed.

			- Kubernetes/Cloud Examples:
				- Kubernetes Service of type LoadBalancer (when provisioned by a cloud provider like Azure Standard Load Balancer or AWS Network Load Balancer (NLB)).
				- kube-proxy itself in IPVS mode or iptables mode largely operates at L4 for Service routing.

		- Layer 7 (Application Layer - HTTP/HTTPS, gRPC, etc.):
			- Operation: Works at the application protocol level (e.g., HTTP). It can inspect the content of the messages.
			- Visibility: Can read and understand application-level data like:
				- HTTP Host headers (e.g., Host: flipkart.com)
				- HTTP Paths (e.g., /product, /user/profile)
				- HTTP Methods (GET, POST)
				- Cookies, Query parameters
				- Other headers

			- Use Cases: 
				- Content-based routing / Path-based routing: Directing traffic to different backend services based on the URL path (e.g., example.com/api/* to API service, example.com/ui/* to UI service).
				- Host-based routing: Directing traffic based on the domain name (e.g., app1.example.com to app1 service, app2.example.com to app2 service).
				
			- Kubernetes/Cloud Examples:
				- Kubernetes Ingress controllers (Nginx Ingress, Traefik, Istio Gateway).
				- Cloud-native L7 Load Balancers like AWS Application Load Balancer (ALB), Azure Application Gateway, Google Cloud HTTP(S) Load Balancer.

2.) Pods in EKS Accessing AWS Services (The IRSA Deep Dive)

	- If your Pods running inside an Amazon EKS (Elastic Kubernetes Service) cluster need to access AWS services (like S3, DynamoDB, SQS, or even make API calls to create ALBs via the AWS Load Balancer Controller), they need AWS credentials.

	- Option 1: Worker Node IAM Role (The Less Secure, Legacy Approach)
		- How it works: You create an IAM Role and attach it to the EKS Cluster Node Group that serve as your EKS worker nodes. This role has policies granting permissions to various AWS services.
		- The kubelet on the worker node (and thus any Pod running on that node) can then inherit these permissions by querying the EC2 instance metadata service.

		- Drawbacks:
			- Overly Permissive: All Pods running on a particular node get all the permissions granted by the node's IAM role. This violates the principle of least privilege. A compromised Pod could potentially abuse all permissions of the node.
			- Difficult to Audit: Hard to track which specific Pod/application used which permission.

	- Option 2: IRSA - IAM Roles for Service Accounts (The Preferred, Secure Approach)
		- IRSA allows you to associate an AWS IAM Role directly with a Kubernetes Service Account. Pods that use this Service Account can then assume the associated IAM Role and get fine-grained, specific permissions.

		- Core Concepts & Setup Flow:
			- EKS Cluster OIDC Provider:
				(check the AWS Load Balancer Controller installation Guide to perform the steps, read this for the explanation)
				- Your EKS cluster has an OpenID Connect (OIDC) issuer URL. This URL acts as an identity provider (IdP).
				- You need to enable this in your EKS cluster if not already done. This OIDC provider issues tokens that AWS STS (Security Token Service) can verify.
				- Find it via EKS console.
				- Create an IAM OIDC Identity Provider in AWS IAM
				- Create an IAM Role with a Trust Policy for the OIDC Provider, this is the IAM Role your Pods will assume.
				- Permissions Policy: Attach policies that grant the specific, minimal permissions your application (running in the Pod) needs (e.g., s3:GetObject on a specific bucket).
				- Trust Relationship (Crucial Part - "Web Identity Trusted Entity"):
					- The "Principal" in the trust policy will be the ARN of the OIDC provider created above
					- It uses a Condition to scope down which Kubernetes Service Account can assume this role, 
							- The :sub condition ensures only tokens issued for that specific Service Account can assume the role.
							- The :aud condition ensures the token was intended for STS.
				- Create a Kubernetes Service Account (KSA):
					- If it doesn't exist, create it in your desired namespace:
						- kubectl create serviceaccount YOUR_K8S_SERVICE_ACCOUNT_NAME -n YOUR_NAMESPACE
				- Annotate the Kubernetes Service Account:
					- Link the KSA to the IAM Role by adding an annotation:
						- kubectl annotate serviceaccount YOUR_K8S_SERVICE_ACCOUNT_NAME -n YOUR_NAMESPACE eks.amazonaws.com/role-arn=arn:aws:iam::ACCOUNT_ID:role/YOUR_IAM_ROLE_NAME
				- Configure Your Pod/Deployment to Use the Service Account by specifying 
					- serviceAccountName: YOUR_K8S_SERVICE_ACCOUNT_NAME # This should be done in the deployment manifest file

		- How IRSA is Used in AWS Load Balancer Controller:
			- You create an IAM Role (e.g., AmazonEKSLoadBalancerControllerRole) with the necessary policies (AWS provides a recommended policy).
			- You configure its trust policy to allow the Service Account used by the AWS Load Balancer Controller Pods to assume this role (following the IRSA trust policy structure).
			- When you deploy the AWS Load Balancer Controller (e.g., via Helm), you specify the Service Account name, and this Service Account is created and then annotated with the ARN of AmazonEKSLoadBalancerControllerRole.
			- The controller Pods then use IRSA to obtain temporary AWS credentials, allowing them to make API calls to AWS to manage ALBs/NLBs on your behalf.

----------------------------------------------------------------------------------------------------

Session 5:
-----------

1.) Helm:

	- The Problem: Managing Kubernetes Applications Across Environments

		- When we start deploying our microservices to Kubernetes, we begin by writing manifest files. A typical microservice requires a set of these files to describe its desired state.

		- Manifests per Microservice:

			- deployment.yaml: Defines the Pod specification, number of replicas, and update strategy.

			- service.yaml: Exposes the application pods as a stable network service.

			- configmap.yaml: Manages non-sensitive configuration data, like API endpoints or feature flags.

			- secret.yaml: Manages sensitive data like database passwords or API keys.

			- persistentvolumeclaim.yaml: Requests stateful storage for databases or file uploads.

		- This process works well for a single deployment into a development (Dev) environment. However, real-world software delivery requires promoting applications through multiple stages.

		- Standard Delivery Pipeline:

			- For example: Dev Env -> QA Env -> UAT Env -> Production Env

	- The Challenge: Environment-Specific Configuration

		- The core problem is that the Kubernetes manifests are not identical across these environments. Each environment has unique configuration requirements.

		- Examples of Environmental Differences:

			- Replica Count: Dev might have 1 replica, while Prod needs 10+ for high availability.

			- Image Tags: Dev uses feature-branch images (feature-new-login-xyz), while Prod must only use stable, tagged releases (v2.5.1).

			- Configuration (ConfigMaps): The database hostname for Dev (dev-db.internal) is different from Prod (prod-rds-instance.aws.com).

			- Secrets: Each environment has its own unique database passwords and API keys.

			- Resource Limits: Prod applications require significantly more CPU and memory allocated than Dev applications.

		- This leads to a critical question: How do we manage these manifest variations without creating a maintenance overhead?

			- Common (but Flawed) Approaches:

				- Separate Folders: Creating dev/, qa/, and prod/ folders, each with a full copy of the manifests.

					- Problem: This leads to massive duplication. A simple change to a shared label or annotation must be manually applied to every folder, which is slow and extremely error-prone.

				- CI/CD Scripting: Using tools like Jenkins to find-and-replace placeholders in template files.

					- Problem: This makes your CI/CD pipeline complex and brittle. The logic for your application's configuration is now hidden inside scripts instead of being declaratively managed in a repository.

		- So a need for Kubernetes-native solution designed for packaging applications and managing their configurations is clearly identified.

	- Solution: Helm - The Package Manager for Kubernetes

		- Helm solves the packaging and configuration problem.

		- Helm is a tool that streamlines installing and managing Kubernetes applications. It uses a packaging format called charts.

		- Core Helm Concepts:

			- Chart: A Helm package. It's a collection of files in a directory that describes a related set of Kubernetes resources. This is where we create our templates.

			- Values: A file (typically values.yaml) that provides the default configuration for a chart. These are the values we will override for each environment (values-dev.yaml or values-prod.yaml etc.)

			- Release: An instance of a chart running in a Kubernetes cluster. A single chart can be installed multiple times, creating unique releases.

		- How Helm Solves Our Problem

			- With Helm, we create one chart for our microservice. Instead of hardcoding values, we use placeholders.

				- Example: Templated deployment.yaml

						apiVersion: apps/v1
						kind: Deployment
						metadata:
						  name: {{ .Release.Name }}-deployment
						spec:
						  replicas: {{ .Values.replicaCount }} # Placeholder for replica count
						  template:
							spec:
							  containers:
								- name: my-app
								  image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}" # Placeholders
								  env:
									- name: DB_HOST
									  value: {{ .Values.config.databaseHost }}


				- Now, we manage the differences with small, environment-specific values files.

					- Example: Environment-Specific Value Files

						- values-dev.yaml
						
								replicaCount: 1
								image:
								  tag: "feature-xyz"
								config:
								  databaseHost: "dev-db.internal"

						- values-prod.yaml

								replicaCount: 10
								image:
								  tag: "v2.5.1"
								config:
								  databaseHost: "prod-rds-instance.aws.com"

				- Deployment Command:

					- To deploy to production, we combine the chart with the production values file.

						- helm upgrade --install my-app ./my-chart -f values-prod.yaml

					- To deploy to dev, we combine the chart with the dev values file.

						- helm upgrade --install my-app ./my-chart -f values-dev.yaml

	- Key Benefits:

		- DRY (Don't Repeat Yourself): The application's structure is defined once. Only the configuration values change.

		- Single Command Operations: Deploy or upgrade an entire multi-resource application in one shot.

		- Reliable Rollbacks: If a new release is faulty, you can instantly roll back to a previously deployed version with helm rollback <release-name>.

----------------------------------------------------------------------------------------------------

Session 6:
-----------

1.) A Better Process: GitOps - Git as the Single Source of Truth

	- Helm solved our packaging problem. GitOps solves our deployment process problem.

	- GitOps is an operational framework that uses a Git repository as the single source of truth for declarative infrastructure and applications. 
	
	- The goal is to automate the deployment process by ensuring that the live state of your cluster always matches the state defined in Git.

	- Core Principles of GitOps:

		- Declarative: The entire desired state of your system is described declaratively in a Git repository (e.g., using Helm charts or plain YAML).

		- Versioned and Immutable: Git is the source of truth, giving you a complete audit trail of all changes, the ability to revert changes, and clear ownership.

		- Pulled Automatically: An automated agent in the cluster pulls the desired state from Git and applies it.

		- Continuously Reconciled: The agent continuously observes the cluster's state and corrects any drift from the state defined in Git.

2.) The Tool: ArgoCD - A Declarative GitOps Controller

	- ArgoCD is the agent that lives in your cluster and makes the GitOps workflow a reality.

	- How ArgoCD Works:

		- You configure an ArgoCD Application resource, telling it which Git repository to monitor.

		- ArgoCD fetches the manifests from the Git repository.

		- It compares the state defined in Git (the "desired state") with the resources actually running in the cluster (the "live state").

		- If there is a difference, ArgoCD reports the application as OutOfSync.

		- It can then automatically (or with a manual click) synchronize the cluster, applying the necessary changes to match the state in Git.
		
----------------------------------------------------------------------------------------------------

Session 7:
-----------

1.) Advanced Storage Concepts in Kubernetes & AWS

	- Revisiting Storage Types (AWS Context)

		- Amazon S3 (Simple Storage Service): Object Storage

			- Nature: Object-based storage. You store data as "objects" (files) within "buckets" (containers for objects).

			- Key Characteristics:

				- Highly scalable, durable, and available.

				- Accessed via APIs (HTTP/S endpoints), AWS SDKs, or CLI.

				- Not a block storage device: You cannot "mount" an S3 bucket directly to an EC2 instance or a Kubernetes Pod as if it were a local filesystem drive in the traditional sense.

			- Use Cases: Storing backups, static website assets, application artifacts, data lakes, media files.

		- Interaction with Kubernetes: Pods can interact with S3 using AWS SDKs (leveraging IRSA for credentials) to upload/download objects. Tools like s3fs-fuse can simulate a mount but it's not native block storage and has performance implications.

	
	- Amazon EFS (Elastic File System): Network File System (NFS)

		- Nature: A managed NFS (Network File System) service provided by AWS. It provides shared file storage.

		- Key Characteristics:

			- Shared Access: Multiple EC2 instances (and thus Kubernetes Pods running on those EC2 worker nodes) can mount and access the same EFS filesystem concurrently.

			- Scalability & Elasticity: Storage capacity scales automatically as you add or remove files. You pay for what you use.

			- Mountable: Can be mounted to EC2 instances (and Pods via a PVC) in multiple Availability Zones within a region.


		- Scenario: You have 10 application servers (or Pods) that all need to read from and write to a common set of files (e.g., shared configuration, web content, user-uploaded content for a CMS).

			- You create an EFS filesystem.

			- You create EFS mount targets in the subnets/Availability Zones where your EC2 instances/worker nodes reside.

			- EC2 instances mount the EFS filesystem to a local directory (e.g., /data).

			- In Kubernetes, you'd typically use the AWS EFS CSI Driver to provision PersistentVolumes (PVs) backed by EFS and then PersistentVolumeClaims (PVCs) for your Pods.

		- Use Cases in Kubernetes: Shared content for web servers, CI/CD build artifacts, machine learning training data, home directories.


2.) Understanding Kubernetes hostPath Volumes (Use with Extreme Caution)

	- hostPath Volume Type: Mounts a file or directory from the host node's filesystem directly into a Pod.

	- Problem Scenario:

		- Your cluster has 4 worker nodes (WN-1, WN-2, WN-3, WN-4).

		- You use a hostPath volume for a Pod, say pointing to /mnt/data on the host.

		- If the Pod is scheduled on WN-1: Data written by the Pod will be stored in /mnt/data on WN-1.

		- If that Pod dies and a new instance is scheduled on WN-2: The new Pod will try to access /mnt/data on WN-2. This directory will either not exist or will contain different data than what was on WN-1. The data is not truly persistent or shared across the cluster.

		- This is why hostPath is Generally Bad for Application Data as the Data is tied to a specific node.


3.) The Critical Link: Volume Availability Zones and Cluster Node Availability Zones (Important Interview Point)

	- Background: Many cloud block storage solutions (like AWS EBS, Azure Managed Disks, GCP Persistent Disks) are zonal resources. This means an EBS volume exists in a specific Availability Zone (AZ) (e.g., us-west-2a).

	- The Constraint: An EC2 instance (and therefore also a Kubernetes worker node running on that EC2 instance) can only mount an EBS volume if both the EC2 instance and the EBS volume are in the SAME Availability Zone. You cannot mount an EBS volume from us-west-2a to an EC2 instance in us-west-2b.

	- Implications for Kubernetes Storage Provisioning & Scheduling:

		- Static Provisioning: If you manually create an EBS volume (PV) in us-west-2a and a Pod requests it via a PVC, the Kubernetes scheduler must place that Pod on a worker node running in us-west-2a. If no suitable node exists in that AZ with enough resources, the Pod will remain Pending.

		- Dynamic Provisioning (with Storage Class): When a StorageClass uses a CSI driver (e.g., AWS EBS CSI Driver) to dynamically provision an EBS volume for a PVC:

			- The CSI driver often uses topology-aware scheduling.

			- When a Pod requests a PVC, the scheduler first picks a node for the Pod (considering resource requests, taints, tolerations, affinity, etc.).

			- Once a node is tentatively selected (say, in us-west-2b), the CSI driver is then instructed to provision the EBS volume in that specific AZ (us-west-2b).

			- This ensures the dynamically created volume is accessible to the node where the Pod will run.

	- Why this is Important for Interviews:

		- Demonstrates understanding of fundamental cloud infrastructure constraints.

		- Explains potential reasons for Pods being stuck in Pending state (e.g., "no nodes available to schedule pod, 1 node(s) had volume node affinity conflict").

		- Highlights the importance of designing your cluster and storage strategy with AZs in mind for high availability and fault tolerance. For HA, you'd want worker nodes and potentially replicated data (if using appropriate storage solutions like EFS or replicated databases) across multiple AZs.


----------------------------------------------------------------------------------------------------

Session 8:
-----------


1.) Troubleshooting and Advanced Workload Management

	- kubectl events: Your First Stop for Troubleshooting

		- The command kubectl get events (or kubectl events in newer versions) prints a chronological list of events that have occurred in the cluster (or a specific namespace if -n <namespace> is used).

		- Why it's useful:

			- Shows actions taken by controllers (e.g., "Scaled up replica set myapp-rs to 3").

			- Reveals errors and warnings from the scheduler, kubelet, or controllers (e.g., "FailedScheduling", "ImagePullBackOff", "FailedMount").

			- Provides clues when Pods are not starting, Deployments are not rolling out, or other unexpected behavior occurs.

2.) Understanding Kubectl Request Flow & CSI Driver Permissions

	- Let's break down how a request to create a PersistentVolume (backed by AWS EBS) flows and where permissions are checked:

		- Your Action: You, as a user with kubectl access, apply a PersistentVolumeClaim YAML.

		- Authentication & Authorization Path:

			- kubectl to EKS Control Plane (API Server):

				- Your kubectl command (configured with a kubeconfig file) sends an API request to the Kubernetes API Server in the EKS control plane.

				- AWS IAM Level (Authentication for kubectl user): If your kubeconfig uses AWS IAM to authenticate (e.g., via aws eks get-token or an IAM authenticator), AWS IAM first verifies your identity and whether you are allowed to talk to the EKS cluster API. This is often managed via the EKS Access Entries.

				- Kubernetes RBAC Level (Authorization within Kubernetes): Once authenticated to the API Server, Kubernetes RBAC rules are checked. Does your Kubernetes user/group (derived from the IAM authentication) have permission to create PersistentVolumeClaims in the specified namespace?

			- PVC Creation and Dynamic Provisioning:

				- If authorized, the PVC object is created in etcd.

				- The StorageClass specified in the PVC (or the default StorageClass) indicates which provisioner should handle it (e.g., ebs.csi.aws.com).

				- The Kubernetes volume controller sees the unbound PVC and delegates the provisioning task to the appropriate CSI (Container Storage Interface) driver.

			- CSI Driver Action (e.g., AWS EBS CSI Driver):

				- The AWS EBS CSI Driver is itself a set of Pods running in your cluster (typically in the kube-system namespace).

				- IRSA for CSI Driver Pods: These CSI driver Pods are configured with a Service Account that has an IAM Role associated with it via IRSA. This IAM Role grants the CSI driver Pods permissions to make AWS API calls like ec2:CreateVolume, ec2:AttachVolume, ec2:DescribeVolumes, etc.

				- The CSI driver Pod, using its IRSA-vended temporary AWS credentials, calls the AWS EC2 API to create an actual EBS volume in the appropriate Availability Zone (e.g., ap-southeast-1a).

				- Once the EBS volume is created, the CSI driver creates a PersistentVolume object in Kubernetes representing this EBS volume and binds it to your PVC.


3.) StatefulSets: Managing Stateful Applications

	- While Deployments are great for stateless applications, stateful applications (databases, message queues with persistent state, etc.) have specific needs:

		- Stable, Unique Network Identifiers: Pods need persistent hostnames.

		- Stable, Persistent Storage: Each Pod instance needs its own dedicated, persistent storage that "follows" it even if the Pod is rescheduled.

		- Ordered, Graceful Deployment and Scaling: Pods might need to be created, updated, or deleted in a specific order.

		- Ordered, Graceful Termination: Pods need to shut down cleanly, potentially performing cleanup.

	- StatefulSets provide these guarantees.

		- Stable Network Identifiers:

			- Pods managed by a StatefulSet get predictable, unique names based on an ordinal index: <statefulset-name>-0, <statefulset-name>-1, etc.

			- These names are also their hostnames within the cluster.

			- Headless Service: StatefulSets are almost always used with a "headless" Service (clusterIP: None).

			- A headless Service doesn't do load balancing itself and doesn't have a single ClusterIP.

			- Instead, it creates DNS A records for each Pod managed by the StatefulSet, in the format: <pod-name>.<headless-service-name>.<namespace>.svc.cluster.local.

			- Example: For a database cluster (master and 2 readers):

			- StatefulSet db, Headless Service db-hl.

			- Pods: db-0, db-1, db-2.

			- DNS: db-0.db-hl.my-namespace..., db-1.db-hl.my-namespace..., db-2.db-hl.my-namespace...

			- This allows the db-0 (master) Pod to directly address db-1 and db-2 by their stable DNS names for replication or coordination, without needing to discover dynamic Pod IPs.

		- Stable, Persistent Storage:

			- StatefulSets can use a volumeClaimTemplates section in their manifest.

			- For each Pod replica, this template automatically creates a unique PersistentVolumeClaim (and thus a PersistentVolume).

			- Example: db-0 gets data-db-0 PVC, db-1 gets data-db-1 PVC.

			- If db-0 is rescheduled to a different node, its data-db-0 PVC (and the underlying PV) is re-attached, ensuring it gets its original data.

		- Ordered Operations:

			- Deployment/Scaling Up: Pods are created one at a time, in order (0, 1, 2...). The next Pod isn't created until the previous one is Running and Ready.

			- Deletion/Scaling Down: Pods are deleted in reverse order (...2, 1, 0). The next Pod isn't deleted until the previous one has fully terminated.

			- Updates (RollingUpdate): Partitions can be used to control the update process, updating Pods in reverse order from the highest ordinal down.

	- Use Cases for StatefulSets:

		- Relational Databases (e.g., PostgreSQL, MySQL clusters)

		- NoSQL Databases (e.g., Cassandra, MongoDB replica sets)

		- Message Queues with persistent state (e.g., Kafka, RabbitMQ clusters)

		- Any application where instances have unique identities, stable storage per instance, and require ordered operations.


4.) Analogy for Kubernetes RBAC vs. AWS IAM

		-- RBAC:

		Service Account                      - IAM Role
		ClusterRole / Role                   - IAM policy
		ClusterRoleBinding / RoleBinding     - Attach the Policy to the Role 


5.) Examples of CPU-Intensive Applications:

	- Video Encoding/Transcoding: Processing raw video data into different formats/resolutions.

	- Scientific Computations/Simulations: Complex mathematical modeling (e.g., weather forecasting, physics simulations).

	- Data Analytics & Machine Learning Training: Processing large datasets, running complex algorithms.

	- Cryptographic Operations: Intensive encryption/decryption tasks (though often offloaded to hardware).

	- High-Traffic Request Processing (CPU-bound tasks): Web servers performing complex calculations or data manipulation per request rather than I/O-bound tasks.


6.) Examples of Memory-Intensive Applications:

	- In-Memory Databases/Caches: Redis, Memcached, or databases configured to hold large working sets in RAM.

	- Large Data Set Processing: Applications that load significant amounts of data into memory for analysis (e.g., some big data frameworks, genomics analysis).

	- Java Applications (with large heaps): JVMs can be configured with large heap sizes.
	
7.) HPA dependency on CPU resources (requests and limits)for Utilization Calculation:
	
	- The HPA calculates CPU utilization as: (Actual CPU Usage of Pod) / (CPU Request of Pod).

	- If you don't set a CPU request, the HPA cannot accurately determine the utilization percentage to make scaling decisions.

	- So, CPU requests are non-negotiable for HPA based on CPU utilization.

	- CPU Limits for Node Stability and Preventing "Noisy Neighbors":
		
		- Safety Net: CPU limits act as a hard cap. If a Pod (due to a bug, unexpected load, or misconfiguration) starts consuming excessive CPU, the limit prevents it from monopolizing the node's CPU resources and starving other Pods (including critical system Pods or other applications).

		- Predictable Performance: Without limits, a runaway Pod can degrade the performance of all other workloads on that node, potentially leading to cascading failures or node instability.
		
		- HPA Scales Replicas, Not Individual Pod Capacity: HPA adds or removes instances (Pods) of your application. It doesn't increase the CPU capacity of an individual Pod. If one Pod in your HPA-managed deployment goes rogue and has no limit, it can still cause problems before HPA can react or even if HPA has scaled out.

		- While HPA doesn't directly use the limit for its scaling decisions, the limit ensures that individual Pods behave predictably.